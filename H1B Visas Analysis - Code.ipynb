{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATS6103 - Individual Project 3 - Andrea Piolini \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is a data mining analysis of the H1B visa applications that were submitted by employers located in the United States between 2011 and 2016. H1B visa is an employment-based, non-immigrant visa for temporary workers in the United States. For this visa, an employer must offer the applicant a job and apply for the H1B visa petition with the US Immigration Department. Once the applicant has been sponsored by the company, the company needs to file the Labor Condition Application (LCA), which needs to be approved by United States Citizenship and Immigration Services (USCIS). If the USCIS approves it, the applicant needs to go through a lottery, and if he/she passes the lottery, the USCIS needs to review the application before approval. Once the USCIS approves the application, the applicant gets the visa. This dataset contains all the H1B Visa application submitted through the LCA to the USCIS between 2011 and 2016. If the case status says \"Certified\", it means that the applicant was approved to go through the lottery. However, this does not mean that the applicant eventually got the visa, and the dataset does not contain such information. \n",
    "\n",
    "The dataset was retrieved from Kaggle in November 2019, and it was originally scraped from the website of the U.S. Office of Foreign Labor by a Kaggle user. The dataset contains over 3 million rows and 11 columns, which include the job title, the name of the company that sponsored the visa, and its geographic coordinates. The dataset is rather thorough and does not have many missing values. However, it would have been more interesting if it included the data from 2017 and 2018 as well, as it would have been interesting to asses the impact of the Trump administration on the visa application process. \n",
    "\n",
    "As the number of observations is huge, my computer would take a lot of time to run the code and display the graphs. Therefore, most of the analysis was conducted on a random sample of 250,000 observations. The part on Data Science is the only part were the full dataset was analyzed.  \n",
    "\n",
    "### Project's Goal and Structure\n",
    "\n",
    "This analysis seek to answer the following questions:\n",
    "\n",
    "- What were the most sponsored jobs between 2011 and 2016?\n",
    "- What was the distribution of the salaries of the jobs sponsored?\n",
    "- What were the companies that sponsored the largest number of visas?\n",
    "- What were the cities and the states that sponsored more visas?\n",
    "\n",
    "Moreover, as I am a foreign student studying Data Science in the United States, I decided to narrow down the scope of the analysis and just focus on the Data Science visa applications. The final section of this project seeks to answer the questions stated above for the Data Science industry only. \n",
    "\n",
    "This project is divided in the following sections:\n",
    "\n",
    "- Data Cleaning Process and Preliminary Analysis\n",
    "- Part 1: Most Sponsored Jobs\n",
    "- Part 2: Salary Distribution\n",
    "- Part 3: Companies that Sponsored the Largest Number of Visas\n",
    "- Part 4: Sponsored Visas by City and State\n",
    "- Part 5: Analysis of the Data Science Industry\n",
    "- Part 6: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Process and Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "H1B_1 = pd.read_excel('H1B1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_2 = pd.read_excel('H1B2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_3 = pd.read_excel('H1B3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1B_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating the dataframes to create a single dataset \n",
    "frames = [H1B_1, H1B_2, H1B_3]\n",
    "H1B_Visas = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at datatypes\n",
    "H1B_Visas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the data\n",
    "H1B_Visas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data a little bit\n",
    "H1B_Visas.columns = map(str.lower, H1B_Visas.columns) #converting al the column names to lower case\n",
    "H1B_Visas.columns = map(str.title, H1B_Visas.columns) #capitalizing the first letter of each name\n",
    "H1B_Visas = H1B_Visas.drop('App_Number', axis = 1)\n",
    "H1B_Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all the str observations from upper case to camel case\n",
    "H1B_Visas[\"Case_Status\"] = H1B_Visas[\"Case_Status\"].str.title()\n",
    "H1B_Visas[\"Employer_Name\"] = H1B_Visas[\"Employer_Name\"].str.title()\n",
    "H1B_Visas[\"Soc_Name\"] = H1B_Visas[\"Soc_Name\"].str.title()\n",
    "H1B_Visas[\"Job_Title\"] = H1B_Visas[\"Job_Title\"].str.title()\n",
    "H1B_Visas[\"Worksite\"] = H1B_Visas[\"Worksite\"].str.title()\n",
    "H1B_Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the number of NAs\n",
    "H1B_Visas.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping NAs\n",
    "H1B_Visas = H1B_Visas.dropna()\n",
    "H1B_Visas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the column Workiste into the city and the state where the company that sponsored the visa is based \n",
    "H1B_Visas[['Worksite','State']] = H1B_Visas.Worksite.str.split(',', expand=True) \n",
    "H1B_Visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the Year format from float to datetime\n",
    "H1B_Visas['Year'] = H1B_Visas['Year'].astype(int)\n",
    "H1B_Visas.Year = pd.to_datetime(H1B_Visas.Year, format='%Y')\n",
    "H1B_Visas['Year'] = H1B_Visas['Year'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving the State column next to the Workiste column for clarity \n",
    "H1B_Visas = H1B_Visas[['Case_Status', 'Employer_Name', 'Soc_Name', 'Job_Title', 'Full_Time_Position', 'Prevailing_Wage', 'Year', 'Worksite', 'State', 'Lon', 'Lat']]\n",
    "H1B_Visas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying descriptive statistics of the data frame\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x) #suppressing scientific notation\n",
    "H1B_Visas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the maximum salary in the dataset is over 6 billion dollars. This seems quite unlikely, so we are going to explore the data frame a little more to see if there are any other outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying all the rows with a salary higher than $1 million \n",
    "Outliers = H1B_Visas.loc[H1B_Visas['Prevailing_Wage'] >= 1000000]\n",
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the number of rows with a salary higher than $1 million\n",
    "Outliers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also checking salaries lower than $10,000 a year\n",
    "Outliers1 = H1B_Visas.loc[H1B_Visas['Prevailing_Wage'] < 10000]\n",
    "Outliers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the number of rows with salaries lower than $10,000 a year\n",
    "Outliers1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the outliers\n",
    "H1B_Visas = H1B_Visas[H1B_Visas.Prevailing_Wage < 1000000]\n",
    "H1B_Visas = H1B_Visas[H1B_Visas.Prevailing_Wage > 10000]\n",
    "H1B_Visas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "\n",
    "As a first step of the preliminary analysis, we will display some wordclouds to show the most frequent words in the following columns: Job_Title, Worksite, State, and Employer_Name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting a radom sample of 250,000 observation to conduct the analysis on\n",
    "H1BSample = H1B_Visas.sample(250000)\n",
    "H1BSample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word cloud with the most common words for jobs\n",
    "text = \" \".join(title for title in H1BSample.Job_Title)\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Creating a word cloud image:\n",
    "wordcloud = WordCloud(width = 1500, height = 800,       #setting the features of the word cloud\n",
    "                background_color ='pink', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "# Displaing the generated word cloud:\n",
    "plt.figure(figsize = (15, 8), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word cloud with the most common words for locations\n",
    "text = \" \".join(worksite for worksite in H1BSample.Worksite)\n",
    "\n",
    "# Creating a word cloud image:\n",
    "wordcloud = WordCloud(width = 1500, height = 800,       #setting the features of the word cloud\n",
    "                background_color ='pink', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "#Displaing the generated word cloud:\n",
    "plt.figure(figsize = (15, 8), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a word cloud with the most common words for states\n",
    "text = \" \".join(state for state in H1BSample.State)\n",
    "\n",
    "#Creating a word cloud image:\n",
    "wordcloud = WordCloud(width = 1500, height = 800,       #setting the features of the word cloud\n",
    "                background_color ='pink', \n",
    "                stopwords = stopwords,\n",
    "                max_words = 40,\n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "#Displaing the generated word cloud:\n",
    "plt.figure(figsize = (15, 8), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word cloud with the most common words for employers\n",
    "text = \" \".join(employer for employer in H1BSample.Employer_Name)\n",
    "stopwords.update([\"Inc\", \"Corporation\", \"llc\", \"llp\", \"Limited\"]) #updating stop words with corporate suffixes as they are\n",
    "                                                                  #probably the most common words for companies\n",
    "\n",
    "\n",
    "# Creating a word cloud image:\n",
    "wordcloud = WordCloud(width = 1500, height = 800,       #setting the features of the word cloud\n",
    "                background_color ='pink', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(text)\n",
    "\n",
    "# Displaing the generated word cloud:\n",
    "plt.figure(figsize = (15, 8), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compare the number of approved (Certified) or denied visas. The status Certified-Withdrawn means that the application was approved by USCIS but then the employer withdrew it and decided not to go on with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signing in with Plotly token\n",
    "py.sign_in('AndrePiolini', '7WymlJ4rlRgAoVzL8KNI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the final status of the visas\n",
    "FinalStatus = H1BSample.Case_Status.value_counts()\n",
    "FinalStatus = pd.DataFrame(FinalStatus)\n",
    "FinalStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalStatus = FinalStatus[(FinalStatus.T != 1).any()] #eliminating the 0s so they won't appear in the pie chart later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the frequency table on the status of the visas using a pie chart \n",
    "colors = ['Green', 'Orange', 'Red', 'Grey']\n",
    "FinalStatus.plot(kind='pie', colors = colors, figsize = (9,9), shadow = True, startangle = 140, \n",
    "         autopct='%1.1f%%', subplots=True, labels=None, wedgeprops={'linewidth' : 2, 'edgecolor' : \"white\"})\n",
    "labels = ['Certified', 'Certified-Withdrawn', 'Denied', 'Withdrawn']\n",
    "plt.title('Case Status', fontsize=14)\n",
    "plt.legend(labels)\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the case statuses per year\n",
    "StatusByYear = pd.crosstab(H1BSample['Year'], H1BSample['Case_Status'])\n",
    "StatusByYear = pd.DataFrame(StatusByYear)\n",
    "StatusByYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting case statuses per year\n",
    "colors = ['Green', 'Orange', 'Red', 'Grey']\n",
    "ax = StatusByYear.plot(kind = 'bar', color = colors, figsize= (20,10), fontsize = 13)\n",
    "plt.legend(loc='best', fontsize = 13)\n",
    "ax.set_xlabel('Year', fontsize = 13)\n",
    "ax.set_ylabel('Number of Applications', fontsize = 13)\n",
    "plt.xticks(rotation=360)\n",
    "plt.title('Insert Title', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Most Sponsored Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at the Soc_Name column\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "SocNames = H1BSample.groupby('Soc_Name').size()\n",
    "SocNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a look at the Job_Title column\n",
    "JobTitles = H1BSample.groupby('Job_Title').size()\n",
    "JobTitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are over 200.000 different jobs titles, and most of them are the same but spelled differently so Python displays them as if they were different job titles. Considering the low quality of the values in this column, it would be pretty hard to gain significant insights from it. Moreover, considering the huge number of values, cleaning the data would be extremely time consuming. Therefore, in this project I'll only consider the column Soc_Name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the number of times each job got sponsored. Then converting it to a dataframe. \n",
    "SponsoredFreq = H1BSample.Soc_Name.value_counts()\n",
    "SponsoredFreq = pd.DataFrame(SponsoredFreq)\n",
    "SponsoredFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe with the top 20 sponsored jobs and plotting it\n",
    "SponsoredFreq = SponsoredFreq.sort_values(by = 'Soc_Name')\n",
    "Top20 = SponsoredFreq.tail(20)\n",
    "Top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 jobs sponsored \n",
    "ax = Top20.plot(kind = 'barh', color = 'coral', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 13)\n",
    "ax.set_ylabel('Type of Job', fontsize = 13)\n",
    "plt.title('Most Popular Sponsored Jobs', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Salary Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the values in the Prevailing_Wage column from float to int\n",
    "H1B_Visas['Prevailing_Wage'] = H1B_Visas['Prevailing_Wage'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the caracteristics of the Prevailing_Wage column\n",
    "H1BSample['Prevailing_Wage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting an histogram of the prevailing wages\n",
    "H1BSample['Prevailing_Wage'].plot(kind = 'hist', color = 'darkorange', bins = 100, figsize=(13,8), edgecolor='black')\n",
    "plt.xlabel('Prevailing Wages',  fontsize = 13)\n",
    "plt.ylabel('Number of Jobs',  fontsize = 13)\n",
    "plt.title('Histogram of Wages from 2011 to 2016',  fontsize = 16)\n",
    "plt.xticks(np.arange(0, 300000, step= 25000))\n",
    "plt.xlim(0, 300000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a violin plot to see the distribution of the salaries over the years \n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "sns.violinplot( x=H1BSample[\"Prevailing_Wage\"], y=H1BSample[\"Year\"], orient = 'h', palette = \"Oranges_r\", inner = \"quartiles\")\n",
    "sns.set_style(style = 'dark') #setting the style of the grid\n",
    "plt.xlabel('Prevailing Wages',  fontsize = 13)\n",
    "plt.ylabel('Year',  fontsize = 13)\n",
    "plt.title('Wages Distribution per Year',  fontsize = 16)\n",
    "plt.xticks(np.arange(0, 300000, step= 25000))\n",
    "plt.xlim(0, 300000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Companies that Sponsored the Largest Number of Visas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the companies that sponsored the largest number of visas. \n",
    "#Then converting it to a dataframe. \n",
    "Companies = H1BSample.Employer_Name.value_counts()\n",
    "Companies = pd.DataFrame(Companies)\n",
    "Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the top 20 companies\n",
    "Companies = Companies.sort_values(by = 'Employer_Name')\n",
    "Top20Companies = Companies.tail(20)\n",
    "Top20Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 companies\n",
    "ax = Top20Companies.plot(kind = 'barh', color = 'gold', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 16)\n",
    "ax.set_ylabel('Company', fontsize = 16)\n",
    "plt.title('Companies that Sponsored the Largest Number of Visas', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Sponsored Visas by City and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the cities where the largest number of visas was sponsored.\n",
    "#Then converting it to a dataframe. \n",
    "Cities = H1BSample.Worksite.value_counts()\n",
    "Cities = pd.DataFrame(Cities)\n",
    "Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the top 20 cities\n",
    "Cities = Cities.sort_values(by = 'Worksite')\n",
    "Top20Cities = Cities.tail(20)\n",
    "Top20Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 cities\n",
    "ax = Top20Cities.plot(kind = 'barh', color = 'goldenrod', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 16)\n",
    "ax.set_ylabel('City', fontsize = 16)\n",
    "plt.title('Cities that Sponsored the Largest Number of Visas', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting every single visa sponsored on a map of the continental United States. Every dot indicates the company that sponsored the visa, \n",
    "#the job title, the year and the city where the visa was sponsored\n",
    "fig = px.scatter_mapbox(H1BSample, lat=\"Lat\", lon=\"Lon\", hover_name=\"Employer_Name\", hover_data=[\"Case_Status\", \"Job_Title\", \"Year\", \"Worksite\"],\n",
    "                        color_discrete_sequence=[\"orange\"], zoom=3, height=500)\n",
    "#setting the type of map\n",
    "fig.update_layout(mapbox_style=\"carto-darkmatter\")\n",
    "#setting the map's margins\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column called Number of Visas Sponsored\n",
    "H1BSample['Number of Visas Sponsored'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a density map displaying the number of visas sponsored\n",
    "fig = go.Figure(go.Densitymapbox(lat=H1BSample.Lat, lon=H1BSample.Lon, z=H1BSample['Number of Visas Sponsored'], \n",
    "                                 radius=10, zmin = 0, zmax = 500))\n",
    "#setting the type of map, the center and the zoom level\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", mapbox_center_lon=-96.683421, mapbox_center_lat = 39.911756, mapbox_zoom=3)\n",
    "#setting the map's margins\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading the dataset StatFIPS. This dataset was retrieved from one of the lectures and \n",
    "#it will be useful to build a chloropleth map\n",
    "StateFIPS = pd.read_csv(\"StateFIPS.csv\", dtype={'statefips':str})\n",
    "StateFIPS.rename(columns = {StateFIPS.columns[0]: \"State\"}, inplace = True)\n",
    "StateFIPS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1BSample.State = H1BSample.State.astype(str) #making sure the State columns for both data frames are in str format\n",
    "StateFIPS.State = StateFIPS.State.astype(str)\n",
    "H1BSample['State'] = H1BSample['State'].str.strip() #removing any unnecessary space in the State colums for both data frame. \n",
    "StateFIPS['State'] = StateFIPS['State'].str.strip() #unnecessray spaces will prevent the two data frames from merging properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the two data frames\n",
    "H1BFips = pd.merge(H1BSample, StateFIPS, on='State')\n",
    "H1BFips = pd.DataFrame(H1BFips)\n",
    "H1BFips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of H1BFips\n",
    "H1BFips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new data frame ByState showing the number of sponsorizations by state abreviation\n",
    "ByState = H1BFips.groupby('stateabrev').sum()\n",
    "ByState = pd.DataFrame(ByState)\n",
    "ByState.reset_index(level=0, inplace=True) #transforming the index colum stateabrev into a normal column\n",
    "ByState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a chloropleth map displaying the number of visas sponsored by state\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations= ByState['stateabrev'], # Spatial coordinates\n",
    "    z = ByState['Number of Visas Sponsored'], # Data to be color-coded\n",
    "    locationmode = 'USA-states', \n",
    "    colorscale = 'Oranges',\n",
    "    autocolorscale=False,\n",
    "    colorbar_title = \"Number of Visas Sponsored\",\n",
    "    zmin = 0, zmax = 20000,\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Number of Visas Sponsored by State',\n",
    "    geo_scope='usa', # limite map scope to the US\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis of the Data Science Industry\n",
    "\n",
    "Mention that this is not going to be done on a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new data frame that only contains jobe related to the Data Science industry by subsetting the H1B_Visas data frame\n",
    "DSOthers = H1B_Visas.copy()\n",
    "DataScience = H1B_Visas[H1B_Visas['Job_Title'].str.contains(\"Data Scientist\" or \"Data Analytics\" or \"Data Analyst\")]\n",
    "DataScience.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary analysis, I want to see what percentage of the overall jobs sponsored data science jobs account for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculatingwhat percentage of the overall jobs sponsored data science jobs account for\n",
    "NumberDSJobs = len(DataScience)\n",
    "NumberOthers = len(H1B_Visas) - len(DataScience)\n",
    "PercJob = (NumberDSJobs/NumberOthers) * 100\n",
    "PercJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column called Number of Visas Sponsored\n",
    "DataScience['Number of Visas Sponsored'] = 1\n",
    "DataScience.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table by year to see the number of the data science visas sponsored over the years\n",
    "ByYear = pd.crosstab(DataScience['Year'], DataScience['Number of Visas Sponsored'])\n",
    "ByYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the numbers of data science applications over time\n",
    "ax = ByYear.plot(colors = 'SpringGreen', figsize= (13,8), fontsize = 13, linewidth = 3.0, legend = None)\n",
    "ax.set_xlabel('Year', fontsize = 13)\n",
    "ax.set_ylabel('Number of Applications', fontsize = 13)\n",
    "plt.title('Number of Data Science Applications Over Time', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the caracteristics of the Prevailing_Wage column for the Data Science data frame \n",
    "DataScience['Prevailing_Wage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting an histogram of the prevailing wages for the Data Science data frame \n",
    "DataScience['Prevailing_Wage'].plot(kind = 'hist', color = 'SpringGreen', bins = 50, figsize=(13,8), edgecolor='black')\n",
    "plt.xlabel('Prevailing Wages',  fontsize = 13)\n",
    "plt.ylabel('Number of Jobs',  fontsize = 13)\n",
    "plt.title('Histogram of Wages for Data Science Jobs from 2011 to 2016',  fontsize = 16)\n",
    "plt.xticks(np.arange(0, 300000, step= 25000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a violin plot to see the distribution of the wages in Data Science over time\n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "sns.violinplot( x=DataScience[\"Prevailing_Wage\"], y=DataScience[\"Year\"], orient = 'h', palette = \"Greens\", inner = \"quartiles\")\n",
    "sns.set_style(style = 'dark')\n",
    "plt.xlabel('Prevailing Wages',  fontsize = 13)\n",
    "plt.ylabel('Year',  fontsize = 13)\n",
    "plt.title('Wages Distribution per Year',  fontsize = 16)\n",
    "plt.xticks(np.arange(0, 300000, step= 25000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays the Data Science companies that sponsored the largest number of visas. \n",
    "#Then converting it to a dataframe. \n",
    "DataScienceCompanies = DataScience.Employer_Name.value_counts()\n",
    "DataScienceCompanies = pd.DataFrame(DataScienceCompanies)\n",
    "DataScienceCompanies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the top 20 companies that sponsored the largest amount of visas\n",
    "DataScienceCompanies = DataScienceCompanies.sort_values(by = 'Employer_Name')\n",
    "Top20DSCompanies = DataScienceCompanies.tail(20)\n",
    "Top20DSCompanies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 companies\n",
    "ax = Top20DSCompanies.plot(kind = 'barh', color = 'SpringGreen', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 16)\n",
    "ax.set_ylabel('Company', fontsize = 16)\n",
    "plt.title('Companies that Sponsored the Largest Number of Visas in the Data Science Industry', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall results of the graph displayed above make sense, as  most of the companies that sponsor the largest number of visas for data scientist are well-known, multinational tech companies. However, I was surprised Amazon is not among them. I am therefore curious to see how many visas Amazon sponsored in the data science field from 2011 and 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the visas sponsored by Amazon in the data science field\n",
    "DataScience.loc[DataScience['Employer_Name'].str.contains(\"Amazon\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Amazon did not sponsor any visa in the data science field between 2011 and 2016. This is quite surprising, let's take a look at the jobs sponsored by Amazon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the visas sponsored by Amazon in other fields\n",
    "AmazonJobs = H1B_Visas.loc[H1B_Visas['Employer_Name'].str.contains(\"Amazon\")]\n",
    "AmazonJobs = AmazonJobs.Job_Title.value_counts()\n",
    "AmazonJobs = pd.DataFrame(AmazonJobs)\n",
    "AmazonJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the top 20 job titles\n",
    "AmazonJobs = AmazonJobs.sort_values(by = 'Job_Title')\n",
    "Top20Amazon = AmazonJobs.tail(20)\n",
    "Top20Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 job titles\n",
    "ax = Top20Amazon.plot(kind = 'barh', color = 'cornflowerblue', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 16)\n",
    "ax.set_ylabel('Job Title', fontsize = 16)\n",
    "plt.title('Top 20 Job Titles Sponsored by Amazon Between 2011 and 2016', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the jobs that Amazon sponsored the most are software development engineers, product managers, and program managers. These two last job titles are rather vague, and data scientists might very well work as program and product managers. Consequently, we can not be extremely sure that Amazon did not sponsor data science jobs whatsoever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a frequency table that displays cities where the largest number of Data Science visas was sponsored. \n",
    "#Then converting it to a dataframe. \n",
    "DataScienceCities = DataScience.Worksite.value_counts()\n",
    "DataScienceCities = pd.DataFrame(DataScienceCities)\n",
    "DataScienceCities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the top 20 cities\n",
    "DataScienceCities = DataScienceCities.sort_values(by = 'Worksite')\n",
    "Top20DSCities = DataScienceCities.tail(20)\n",
    "Top20DSCities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the top 20 cities\n",
    "ax = Top20DSCities.plot(kind = 'barh', color = 'DarkGreen', figsize= (20,10), fontsize = 13, legend = None)\n",
    "ax.set_xlabel('Number of Jobs Sponsored', fontsize = 16)\n",
    "ax.set_ylabel('City', fontsize = 16)\n",
    "plt.title('Cities that Sponsored the Largest Number of Visas in the Data Science Industry', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting every single Data Science visa sponsored on a map of the continental United States. Every dot indicates the company that sponsored the visa, \n",
    "#the job title, the year and the city where the visa was sponsored\n",
    "fig = px.scatter_mapbox(DataScience, lat=\"Lat\", lon=\"Lon\", hover_name=\"Employer_Name\", hover_data=[\"Case_Status\", \"Job_Title\", \"Year\", \"Worksite\"],\n",
    "                        color_discrete_sequence=[\"SpringGreen\"], zoom=3, height=500)\n",
    "#setting the type of map\n",
    "fig.update_layout(mapbox_style=\"carto-darkmatter\")\n",
    "#setting the map's margins\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a density map displaying the number of Data Science visas sponsored\n",
    "fig = go.Figure(go.Densitymapbox(lat=DataScience.Lat, lon=DataScience.Lon, z=DataScience['Number of Visas Sponsored'], \n",
    "                                 radius=10, zmin = 0, zmax = 10))\n",
    "#setting the type of map, the center and the zoom level\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", mapbox_center_lon=-96.683421, mapbox_center_lat = 39.911756, mapbox_zoom=3)\n",
    "#setting the map's margins\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new data frame that contains the state fips to make a chloropleth\n",
    "DataScience.State = DataScience.State.astype(str) #deleting unnecessary spaces in the State column \n",
    "DataScience['State'] = DataScience['State'].str.strip()\n",
    "\n",
    "#merging the two data frames\n",
    "DataScienceFips = pd.merge(DataScience, StateFIPS, on='State')\n",
    "DataScienceFips = pd.DataFrame(DataScienceFips)\n",
    "DataScienceFips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new data frame DSByState showing the number of sponsorizations in the data science industry by state abreviation\n",
    "DSByState = DataScienceFips.groupby('stateabrev').sum()\n",
    "DSByState = pd.DataFrame(DSByState)\n",
    "DSByState.reset_index(level=0, inplace=True) #transforming the index colum stateabrev into a normal column\n",
    "DSByState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a chloropleth map \n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations= DSByState['stateabrev'], # Spatial coordinates\n",
    "    z = DSByState['Number of Visas Sponsored'], # Data to be color-coded\n",
    "    locationmode = 'USA-states', \n",
    "    colorscale = 'Greens',\n",
    "    autocolorscale=False,\n",
    "    colorbar_title = \"Number of Visas Sponsored\",\n",
    "    zmin = 0, zmax = 300,\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Number of Visas Sponsored in the Data Science Industry by State',\n",
    "    geo_scope='usa', # limite map scope to the US\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Complete Dataset\n",
    "\n",
    "- The vast majority of the H1B Visa application were approved. Only approximately 3% of the application was denied. \n",
    "- The most sponsored jobs between 2011 and 2016 were jobs related to computers, software and technology in general. Finance-related jobs such as accountants and financial analysts were also in the top 20 most sponsored jobs. \n",
    "- The wage distribution for all jobs between 2011 and 2016 is right skewed. When looking at the wage distribution for each year, we can see that they are right skewed as well and overall rather similar to each other, although there was a slow, yet steady increase of the first quartile, median, and third quartile for each year. \n",
    "- The two companies that sponsored the largest number of visas are Infosys and Tata Consultancy Services, two Indian multinational consulting firms. Overall, consulting firms are the ones that sponsored the largest number of visas, followed by IT and tech giants such as Microsoft and Amazon. \n",
    "- Eight cities among the top 20 cities that sponsored the largest number of visas are located in California. However, New York City is the city with the highest number of applications. Big tech and business hubs such as Seattle, Houston, and Atlanta are also among the top 20 cities. \n",
    "- California, New York, and Texas are the states where the majority of the visas were sponsored. Additionally, a significant number of applications were filed in Illinois, New Jersey, and Washington state. \n",
    "\n",
    "### Data Science\n",
    "\n",
    "- The applications to sponsor jobs in the data science field spiked between 2011 and 2016; however, data science jobs represented only 0.1 percent of the total jobs sponsored. This indicates that data science is becoming more popular and it is a booming field, but it is also a relatively new field and the number of data scientist is still relatively low. It would have been interesting to see if the trend was confirmed in 2017 and 2018. Most likely it was. \n",
    "- The average salary of a data scientist is higher than the average salary of the total jobs sponsored.\n",
    "- The companies that sponsored the largest number of data science jobs are well-known, tech giants such as Microsoft, Uber, and Facebook. \n",
    "- Eleven cities out of the top 20 that sponsored the largest number of visas in data science are located in California. Unsurprisingly then, California is the state that sponsored most visas in the Data Science industry. Other states that sponsored quite a lot in the industry are New York, Washington, Illinois, and Texas. \n",
    "\n",
    "### 2017-2018 Trends\n",
    "\n",
    "As stated before, one limitation of this dataset is that it does not contain the data for 2017 and 2018. However, I doubt that the major trends we saw in this analysis changed. The most sponsored jobs are most likely the ones related to software development, computers and consulting, and the companies that sponsor the majority of the visas are probably multinational companies operating in consulting, IT, and technology in general. Location-wise, the situation has probably not changed either. Cities like San Francisco, Seattle, and New York City are still major business and technology hubs where companies keep sponsoring a significant amount of visas every year. The trends in the Data Science industry most likely remained the same through 2017 and 2018. The only thing that might have changed significantly is the number of the application for Data Science, which I expect to have grown tremendously in the last two years. \n",
    "\n",
    "Overall, the only major difference between the applications filed between 2011 and 2016 and the ones filed in 2017-2018 might be the denial rate. Indeed - according to several media outlets - the Trump Administration reportedly has been \"aggressively\" denying applications for H1B visas since Trump took office in January 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
